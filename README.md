# Credit Card Fraud Detection

An investigation of the **Credit Card Fraud Detection dataset** from Kaggle.  

Original dataset and variable summary:  
ðŸ‘‰ [Kaggle â€“ Credit Card Fraud Detection](https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud)

---

## Goal
- Explore and analyze the dataset  
- Identify potential issues or limitations  
- Build and evaluate machine learning models to predict fraudulent financial transactions  

---

## Repository Structure

- **`fraud_detection_initial_screen.ipynb`**  
  Main notebook containing exploratory analysis and modeling.

- **`breakdown_plots/`**  
  Contains plots generated during analysis, primarily histograms.  
  - **`Histograms/`** â€“ Histograms of each variable in the dataset (similar to Kaggle, but recreated for hands-on analysis).  
  - **`Histograms_Downsample_Split/`** â€“ Histograms after downsampling, split by class:  
    - `0` â†’ Non-fraudulent transactions  
    - `1` â†’ Fraudulent transactions  

---

## Initial Observations

- **Scope:** Data covers only 2 days and does not include individual client separation â†’ limits long-term behavioral insights.  
- **Preprocessing:** Most features are PCA components derived from original transaction variables (e.g., type, origin country, destination country, business/personal account).  
- **Imbalance:**  
  - Non-fraudulent: **284,315**  
  - Fraudulent: **492**  
  - Fraudulent transactions make up only **0.17%** of the dataset.  
- **Quality:** No missing values. Dataset has **31 features** and **284,807 rows**.  

If original categorical variables were available, I would:  
1. One-hot encode categorical features.  
2. Perform data quality checks and feature engineering (e.g., international vs. domestic transactions).  
3. Apply PCA to reduce dimensionality and highlight differentiating features.  

---

## Handling Class Imbalance

- **Chosen method:** Downsampling â†’ balanced dataset with 492 fraudulent + 492 non-fraudulent transactions (984 total).  
- **Rationale:**  
  - Training size is sufficient (~885 samples after 90/10 train-test split).  
  - Other options include upsampling (synthetic minority generation) or applying cost-sensitive metrics (e.g., F1 score, class weighting).  

---

## Feature Exploration & Selection

- Plotted downsampled histograms split by fraud class.  
  - Example of a **differentiating feature**:  
    ![V4 Histogram Split](https://github.com/bgoodman90/Credit_Card_Fraud_Detection/blob/main/breakdown_plots/Histograms_Downsample_Split/V4_hist.png)  
  - Example of a **non-differentiating feature**:  
    ![V22 Histogram Split](https://github.com/bgoodman90/Credit_Card_Fraud_Detection/blob/main/breakdown_plots/Histograms_Downsample_Split/V22_hist.png)  

- Used a **Random Forest classifier** to obtain feature importances:  
  ![Feature Importance](https://github.com/bgoodman90/Credit_Card_Fraud_Detection/blob/main/feature_scores.png)  

- Selected top **17 features** (cutoff at feature `V19`) for modeling.  

---

## Modeling & Results

Models were tuned via randomized CV grid search (stratified 5-fold, balanced classes, fixed random seed).  

**Cross-Validation F1-Score (avg. over 5 folds on balanced data set):**
- Random Forest â†’ **0.9378**  
- XGBoost â†’ **0.9425**  
- Logistic Regression â†’ **0.9426**  
- Bernoulli Naive Bayes â†’ **0.9036**  
- Support Vector Classifier (SVC) â†’ **0.9393**  
- Majority Voting (XGBoost + Logistic Regression + SVC) â†’ **0.9427 +/- 0.0098**
- Still experimenting with Neural Network, currently close to 0.88 F1-Score on test set. 

**Cross-Validation Accuracy (avg. over 5 folds on balanced data set):**
- Random Forest â†’ **93.93%**  
- XGBoost â†’ **94.35%**  
- Logistic Regression â†’ **94.46%**  
- Bernoulli Naive Bayes â†’ **91.18%**  
- Support Vector Classifier (SVC) â†’ **94.12%**  
- Majority Voting (XGBoost + Logistic Regression + SVC) â†’ **94.46% Â± 0.97%**

I performed a quick calculation on the Logistic Regression (best performing model) to determine that AUPRC on the test set is 0.9587.

**Key Takeaways:**
- Models perform consistently around **0.94-0.95 F1-Score** and **94â€“95% accuracy** on the balanced dataset.  
- Results are strong given the dataset limitations and imbalance.
- Logistic Regression performs the best (Majority Voting ties it).
- Future improvements could include:  
  - Combining downsampling + upsampling  
  - Evaluating performance on full (imbalanced) data  
  - Analyzing confusion matrices (false positives vs. false negatives are critical in finance)  

---

## Summary

This quick exploration achieved ~95% accuracy using standard ML models on a balanced dataset. While strong, real-world deployment would require deeper investigation into feature engineering, class imbalance handling, and error trade-offs (false positives vs. false negatives).  
